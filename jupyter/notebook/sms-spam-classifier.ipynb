{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"sms-spam-classifier\")\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000/\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-------+\n",
      "|label|                 sms|                 raw|is_spam|\n",
      "+-----+--------------------+--------------------+-------+\n",
      "|  ham|Dear i have reach...|Dear i have reach...|  false|\n",
      "|  ham|Fighting with the...|Fighting with the...|  false|\n",
      "|  ham|When can ü come out?|When can ü come out?|  false|\n",
      "|  ham|Check with nuerol...|Check with nuerol...|  false|\n",
      "|  ham|Lolnice. I went f...|Lolnice. I went f...|  false|\n",
      "| spam|+123 Congratulati...|+123 Congratulati...|   true|\n",
      "|  ham|No it's waiting i...|No it's waiting i...|  false|\n",
      "|  ham|Maybe westshore o...|Maybe westshore o...|  false|\n",
      "|  ham|You should know n...|You should know n...|  false|\n",
      "|  ham|What's the signif...|What's the signif...|  false|\n",
      "|  ham|Your opinion abou...|Your opinion abou...|  false|\n",
      "|  ham|8 at the latest, ...|8 at the latest, ...|  false|\n",
      "|  ham|Prabha..i'm soryd...|Prabha..i'm soryd...|  false|\n",
      "|  ham|Lol ok your forgi...|Lol ok your forgi...|  false|\n",
      "|  ham|No..jst change ta...|No..jst change ta...|  false|\n",
      "| spam|You are guarantee...|You are guarantee...|   true|\n",
      "|  ham|S:)no competition...|S:)no competition...|  false|\n",
      "| spam|Boltblue tones fo...|Boltblue tones fo...|   true|\n",
      "| spam|Your credits have...|Your credits have...|   true|\n",
      "|  ham|That way transpor...|That way transpor...|  false|\n",
      "+-----+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"s3a://delta/\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "mlflow.pyspark.ml.autolog()\n",
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegexTokenizer_d14a9d749b3a\n",
      "StopWordsRemover_c72f9866d529\n",
      "CountVectorizer_2a332918acd4\n",
      "StringIndexer_9ac9df49d71f\n",
      "VectorAssembler_ad188eb41c06\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    CountVectorizer, StringIndexer, VectorAssembler, Tokenizer, RegexTokenizer, StopWordsRemover)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stages = []\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sms\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "stages += [regexTokenizer]\n",
    "\n",
    "STOPWORDS = stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "remover = StopWordsRemover(stopWords=STOPWORDS, inputCol=\"tokens\", outputCol=\"real_tokens\")\n",
    "stages += [remover]\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"real_tokens\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\n",
    "stages += [cv]\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_num\")\n",
    "stages += [indexer]\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\n",
    "stages += [vecAssembler]\n",
    "\n",
    "for stage in stages:\n",
    "    print(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/07 21:52:04 WARNING mlflow.utils: Truncated the value of the key `StopWordsRemover.stopWords`. Truncated value: `['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing...`\n",
      "2023/03/07 21:52:07 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\"\n",
      "2023/03/07 21:52:07 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\"\n",
      "2023/03/07 21:52:07 WARNING mlflow.pyspark.ml: Model outputs contain unsupported Spark data types: [StructField('tokens', ArrayType(StringType(), True), True), StructField('real_tokens', ArrayType(StringType(), True), True), StructField('token_features', VectorUDT(), True), StructField('features', VectorUDT(), True)]. Output schema is not be logged.\n",
      "2023/03/07 21:52:17 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/conda/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "data = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Train spam:  578\n",
      "      Train ham:   3624\n",
      "      Train ratio: 0.15949227373068434 (spam/ham)\n",
      "      \n",
      "      Test spam:  169\n",
      "      Test ham:   1203\n",
      "      Test ratio: 0.1404821280133001 (spam/ham)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "train, test = data.randomSplit([0.75, 0.25], seed = 42)\n",
    "\n",
    "train_s  = train.where('is_spam').count()\n",
    "train_ns = train.where('not is_spam').count()\n",
    "test_s   = test.where('is_spam').count()\n",
    "test_ns  = test.where('not is_spam').count()\n",
    "print(f'''\n",
    "      Train spam:  {train_s}\n",
    "      Train ham:   {train_ns}\n",
    "      Train ratio: {train_s / train_ns} (spam/ham)\n",
    "      \n",
    "      Test spam:  {test_s}\n",
    "      Test ham:   {test_ns}\n",
    "      Test ratio: {test_s / test_ns} (spam/ham)\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/07 21:55:49 WARNING mlflow.pyspark.ml: Model inputs contain unsupported Spark data types: [StructField('tokens', ArrayType(StringType(), True), True), StructField('real_tokens', ArrayType(StringType(), True), True), StructField('token_features', VectorUDT(), True), StructField('features', VectorUDT(), True)]. Model signature is not logged.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"label_num\")\n",
    "model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+\n",
      "|label_num|prediction|         probability|\n",
      "+---------+----------+--------------------+\n",
      "|      0.0|       0.0|[0.99261568389786...|\n",
      "|      0.0|       0.0|[0.99999976600938...|\n",
      "|      0.0|       0.0|[0.66818703921423...|\n",
      "|      0.0|       0.0|[0.99944368067061...|\n",
      "|      0.0|       0.0|[0.99070125131954...|\n",
      "|      0.0|       0.0|[0.99902425557805...|\n",
      "|      0.0|       0.0|[0.98139506908352...|\n",
      "|      0.0|       0.0|[0.94580670071342...|\n",
      "|      0.0|       0.0|[0.99909646763770...|\n",
      "|      0.0|       0.0|[0.99997739056063...|\n",
      "|      0.0|       0.0|[0.99996787015427...|\n",
      "|      0.0|       0.0|[0.99999999759138...|\n",
      "|      0.0|       0.0|[0.99943190904288...|\n",
      "|      0.0|       0.0|[0.99996803773852...|\n",
      "|      0.0|       0.0|[0.99981908729548...|\n",
      "|      0.0|       0.0|[0.98441744031794...|\n",
      "|      0.0|       0.0|[0.96931889397476...|\n",
      "|      0.0|       0.0|[0.78895353392514...|\n",
      "|      0.0|       0.0|[0.99999891154734...|\n",
      "|      0.0|       0.0|[0.99991025444602...|\n",
      "+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.select(\"label_num\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC:  0.9760141067449719\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label_num\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Test Area Under ROC: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = mlflow.spark.log_model(model, \"naive-bayes\")\n",
    "print(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
