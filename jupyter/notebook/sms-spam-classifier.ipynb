{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"sms-spam-classifier\")\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000/\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-------+\n",
      "|label|                 sms|                 raw|is_spam|\n",
      "+-----+--------------------+--------------------+-------+\n",
      "|  ham|Dear i have reach...|Dear i have reach...|  false|\n",
      "|  ham|Fighting with the...|Fighting with the...|  false|\n",
      "|  ham|When can ü come out?|When can ü come out?|  false|\n",
      "|  ham|Check with nuerol...|Check with nuerol...|  false|\n",
      "|  ham|Lolnice. I went f...|Lolnice. I went f...|  false|\n",
      "| spam|+123 Congratulati...|+123 Congratulati...|   true|\n",
      "|  ham|No it's waiting i...|No it's waiting i...|  false|\n",
      "|  ham|Maybe westshore o...|Maybe westshore o...|  false|\n",
      "|  ham|You should know n...|You should know n...|  false|\n",
      "|  ham|What's the signif...|What's the signif...|  false|\n",
      "|  ham|Your opinion abou...|Your opinion abou...|  false|\n",
      "|  ham|8 at the latest, ...|8 at the latest, ...|  false|\n",
      "|  ham|Prabha..i'm soryd...|Prabha..i'm soryd...|  false|\n",
      "|  ham|Lol ok your forgi...|Lol ok your forgi...|  false|\n",
      "|  ham|No..jst change ta...|No..jst change ta...|  false|\n",
      "| spam|You are guarantee...|You are guarantee...|   true|\n",
      "|  ham|S:)no competition...|S:)no competition...|  false|\n",
      "| spam|Boltblue tones fo...|Boltblue tones fo...|   true|\n",
      "| spam|Your credits have...|Your credits have...|   true|\n",
      "|  ham|That way transpor...|That way transpor...|  false|\n",
      "+-----+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"s3a://delta/\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "mlflow.pyspark.ml.autolog()\n",
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegexTokenizer_09eb1382bc2e\n",
      "StopWordsRemover_8f6f93784e8f\n",
      "CountVectorizer_bb9990a3ed93\n",
      "StringIndexer_38f62958e333\n",
      "VectorAssembler_ca52d7dda6c0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    CountVectorizer, StringIndexer, VectorAssembler, Tokenizer, RegexTokenizer, StopWordsRemover)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stages = []\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sms\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "stages += [regexTokenizer]\n",
    "\n",
    "STOPWORDS = stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "remover = StopWordsRemover(stopWords=STOPWORDS, inputCol=\"tokens\", outputCol=\"real_tokens\")\n",
    "stages += [remover]\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"real_tokens\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\n",
    "stages += [cv]\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_num\")\n",
    "stages += [indexer]\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\n",
    "stages += [vecAssembler]\n",
    "\n",
    "for stage in stages:\n",
    "    print(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/07 00:13:12 WARNING mlflow.utils: Truncated the value of the key `StopWordsRemover.stopWords`. Truncated value: `['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing...`\n",
      "2023/03/07 00:13:16 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\"\n",
      "2023/03/07 00:13:16 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\"\n",
      "2023/03/07 00:13:16 WARNING mlflow.pyspark.ml: Model outputs contain unsupported Spark data types: [StructField('tokens', ArrayType(StringType(), True), True), StructField('real_tokens', ArrayType(StringType(), True), True), StructField('token_features', VectorUDT(), True), StructField('features', VectorUDT(), True)]. Output schema is not be logged.\n",
      "2023/03/07 00:13:32 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/conda/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "data = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Train spam:  302\n",
      "      Train ham:   1951\n",
      "      Train ratio: 0.15479241414659148 (spam/ham)\n",
      "      \n",
      "      Test spam:  107\n",
      "      Test ham:   640\n",
      "      Test ratio: 0.1671875 (spam/ham)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "train, test = data.randomSplit([0.75, 0.25], seed = 42)\n",
    "\n",
    "train_s  = train.where('is_spam').count()\n",
    "train_ns = train.where('not is_spam').count()\n",
    "test_s   = test.where('is_spam').count()\n",
    "test_ns  = test.where('not is_spam').count()\n",
    "print(f'''\n",
    "      Train spam:  {train_s}\n",
    "      Train ham:   {train_ns}\n",
    "      Train ratio: {train_s / train_ns} (spam/ham)\n",
    "      \n",
    "      Test spam:  {test_s}\n",
    "      Test ham:   {test_ns}\n",
    "      Test ratio: {test_s / test_ns} (spam/ham)\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/07 00:17:07 WARNING mlflow.pyspark.ml: Model inputs contain unsupported Spark data types: [StructField('tokens', ArrayType(StringType(), True), True), StructField('real_tokens', ArrayType(StringType(), True), True), StructField('token_features', VectorUDT(), True), StructField('features', VectorUDT(), True)]. Model signature is not logged.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"label_num\")\n",
    "model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+\n",
      "|label_num|prediction|         probability|\n",
      "+---------+----------+--------------------+\n",
      "|      0.0|       0.0|[0.99999931023026...|\n",
      "|      0.0|       0.0|[0.61250491536961...|\n",
      "|      0.0|       0.0|[0.59263392506081...|\n",
      "|      0.0|       0.0|[0.99315906235259...|\n",
      "|      0.0|       0.0|[0.99999962018803...|\n",
      "|      0.0|       0.0|[0.92158442655367...|\n",
      "|      0.0|       0.0|[0.99896842750763...|\n",
      "|      0.0|       0.0|[0.99987070398322...|\n",
      "|      0.0|       0.0|[0.99999999232327...|\n",
      "|      0.0|       0.0|[0.99999990872023...|\n",
      "|      0.0|       0.0|[0.99855559704832...|\n",
      "|      0.0|       0.0|[0.99719387987603...|\n",
      "|      0.0|       0.0|[0.99816190309014...|\n",
      "|      0.0|       0.0|[0.99977386895803...|\n",
      "|      0.0|       0.0|[0.99592511130266...|\n",
      "|      0.0|       0.0|[0.99647928138122...|\n",
      "|      0.0|       0.0|[0.85335475912843...|\n",
      "|      0.0|       0.0|[0.99168697462218...|\n",
      "|      0.0|       0.0|[0.99993816408669...|\n",
      "|      0.0|       0.0|[0.99983930004186...|\n",
      "+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.select(\"label_num\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC:  0.9626022196261682\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label_num\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Test Area Under ROC: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mlflow.models.model.ModelInfo object at 0xffff638eb0a0>\n"
     ]
    }
   ],
   "source": [
    "model_info = mlflow.spark.log_model(model, \"naive-bayes\")\n",
    "print(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
