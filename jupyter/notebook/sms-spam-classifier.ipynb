{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"sms-spam-classifier\")\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000/\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://delta/\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "mlflow.pyspark.ml.autolog()\n",
    "mlflow.create_experiment(\"sms-classifier-\" + datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    CountVectorizer, StringIndexer, VectorAssembler, Tokenizer, RegexTokenizer, StopWordsRemover)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stages = []\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sms\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "stages += [regexTokenizer]\n",
    "\n",
    "STOPWORDS = stopwords.words('english') + ['u', 'Ã¼', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "remover = StopWordsRemover(stopWords=STOPWORDS, inputCol=\"tokens\", outputCol=\"real_tokens\")\n",
    "stages += [remover]\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"real_tokens\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\n",
    "stages += [cv]\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_num\")\n",
    "stages += [indexer]\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\n",
    "stages += [vecAssembler]\n",
    "\n",
    "for stage in stages:\n",
    "    print(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "data = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.75, 0.25], seed = 42)\n",
    "\n",
    "train_s  = train.where('is_spam').count()\n",
    "train_ns = train.where('not is_spam').count()\n",
    "test_s   = test.where('is_spam').count()\n",
    "test_ns  = test.where('not is_spam').count()\n",
    "print(f'''\n",
    "      Train spam:  {train_s}\n",
    "      Train ham:   {train_ns}\n",
    "      Train ratio: {train_s / train_ns} (spam/ham)\n",
    "      \n",
    "      Test spam:  {test_s}\n",
    "      Test ham:   {test_ns}\n",
    "      Test ratio: {test_s / test_ns} (spam/ham)\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"label_num\")\n",
    "model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.select(\"label_num\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label_num\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Test Area Under ROC: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = mlflow.spark.log_model(model, \"naive-bayes\")\n",
    "print(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
